# 404NotSound

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://your-build-url) [![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

[ä¸­æ–‡] | [English](../README.md)

**404NotFoundé›†æˆäº†å¼€æºè¯­éŸ³è¯†åˆ«é¡¹ç›®[SenseVoice](https://github.com/FunAudioLLM/SenseVoice)å’Œ[DeepSeek API](https://api-docs.deepseek.com/)ï¼Œç”¨äºè¯¾å ‚æˆ–è®²åº§è®°å½•ï¼Œä½¿ç”¨DeepSeekæ•´ç†ï¼Œè¾“å‡ºMarkdownæ ¼å¼æ€»ç»“**

[//]: # (![img_1.png]&#40;img_1.png&#41;{:height="50%" width="50%"})
<img src="img_1.png" width = "500" height = "610" alt="overview" align=center />

## ç›®å½•

*   [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
*   [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
    *   [å®¢æˆ·ç«¯](#å®¢æˆ·ç«¯)
    *   [æœåŠ¡å™¨ç«¯](#æœåŠ¡å™¨ç«¯)
*   [å¼€å‘è®¡åˆ’](#å¼€å‘è®¡åˆ’)

## é¡¹ç›®ç®€ä»‹

æ­£å¦‚é¡¹ç›®åæ‰€è¨€ï¼Œ404NotSoundå«è“„åœ°ä»£è¡¨äº†ç»å¸¸åœ¨è¯¾å ‚ä¸Šå‘ç”Ÿçš„ç±»ä¼¼äºâ€œå¯¹ä¸èµ·ï¼Œæˆ‘æ²¡å¬æ¸…æ¥šâ€çš„æƒ…å†µã€‚æœ¬é¡¹ç›®ç›®å‰ä½¿ç”¨PySide6ä½œä¸ºGUIæ¡†æ¶å¼€å‘ï¼ŒSenseVoiceä½œä¸ºè¯­éŸ³è¯†åˆ«å¼•æ“ï¼ŒDeepSeekä½œä¸ºLLMæ•´ç†å¼•æ“ã€‚

ä¾‹å¦‚ï¼š
è¾“å…¥3Blue1Brownçš„è§†é¢‘ï¼ˆæå–å‡ºéŸ³é¢‘ï¼Œåç»­å¯èƒ½ä¼šè€ƒè™‘æ·»åŠ è§†é¢‘æ”¯æŒï¼‰ï¼ŒåŸè§†é¢‘é“¾æ¥
[https://www.bilibili.com/video/BV1xmA2eMEFF](https://www.bilibili.com/video/BV1xmA2eMEFF)ï¼ˆBilibiliï¼‰
[https://youtu.be/LPZh9BOjkQs](https://youtu.be/LPZh9BOjkQs)ï¼ˆYouTubeï¼‰ï¼Œ
SenseVoiceè¯†åˆ«å‡ºçš„æ–‡æœ¬å¦‚ä¸‹ï¼š
<details> 
<summary><font size="4" color="orange">å±•å¼€è¯†åˆ«æ–‡æœ¬</font></summary> 
<pre><code class="text-xl">Imagine you happen across a short movie script that describes a scene between a person and their AI assistant,
the script has what the person asks the AI, but the AI's response has been torn off.
Suppose you also have this powerful magical machine that can take any text and provide
a sensible prediction of what word comes next You could then finish the script by
feeding in what you have to the machine, seeing what it would predict to start
the AI's answer and then repeating this over and over with a growing script completing
the dialogue When you interact with a chatbot, this is exactly what's happening a large
language model is a sophisticated mathematical function that predicts what word comes
next for any piece.ğŸ¼of text instead of predicting one word with certainty, though, what
it does is assign a probability to all possible next words to build a chatbot What you do
is lay out some text that describes an interaction between a user and a hypothetical AI assistant
you add on whatever the user types in as the first part of that interaction and then you have the
model repeatedly predict the next word that such a hypothetical AI assistant would say in response
and that's what's presented to the user in doing,The output tends to look a lot more natural if
you allow it to select less likely words along the way at random, so what this means is even though
the model itself is deterministic, a given prompt typically gives a different answer each time it's
run.Models learn how to make these predictions by processing an enormous amount of text typically
pulled from the internet for a standard human to read the amount of text that was used to train GPT3,
for example, if they read nonstop 24/7, it would take over 2,600 years, larger models since then train
on much, much more.You can think of training a little bit like tuning the dials on a big machine, the
way that a language model behaves is entirely determined by these many different continuous values,
usually called parameters or weights.ğŸ¼Changing those parameters will change the probabilities that
the model gives for the next word on a given input, what puts the large in large language model is 
how they can have hundreds of billions of these parameters.No human ever deliberately sets those
parameters, instead they begin at random, meaning the model just outputs gibberish, but they are
repeatedly refined based on many example pieces of text.One of these training examples could be just
a handful of words, or it could be thousands, but in either case, the way this works is to pass in all
but the last word from that example into the model and compare the prediction that it makes with the
true last word from the example, an algorithm called back propagation is used to tweak all of the
parameters in such a way that it makes the model a little more likely to choose the true last word and
a little less likely to choose all the others.When you do this for many, many trillions of examples, not
only does the model start to give more accurate predictions on the training data, but it also starts to
make more reasonable predictions on text that it's never seen before.Given the huge number of parameters
and the enormous amount of training data, the scale of computation involved in training a large language
model is mind boggling.To illustrate, imagine that you could perform 1 billion editions and multiplications
every single second, how long do you think that it would take for you to do all of the operations involved
in training the largest language models?Do you think it would take a year, maybe something like 10,000
years, The answer is actually much more than that it's well over 100 million years.This is only part of
the story though This whole process is called pre-training The goal of auto completinglet a random passage
of text from the internet is very different from the goal of being a good AI assistant, to address this
chatbots undergo another type of training just as important called reinforcement learning with human feedback Workers flag unhelpful or problematic predictions and their corrections further change the model's parameters, making them more likely to give predictions that users prefer.Looking back at the pretraining though, this staggering amount of computation is only made possible by using special computer chips that are optimized for running many, many operations in parallel known as GPUs. However, not all language models can be easily parallelzed prior to 2017 Most language models would process text one word at a time, but then a team of researchers at Google introduced a new model known as the Transformer.Yeah.ğŸ¼Transformers don't read text from the start to the finish They soak it all in at once in parallel The very first step inside a transformer and most other language models for that matter is to associate each word with a long list of numbers The reason for this is that the training process only works with continuous values so you have to somehow encode language using numbers and each of these list of numbers may somehow encode the meaning of the corresponding word What makes transformers unique is their rely.On a special operation known asten.This operation gives all of these lists of numbers a chance to talk to one another and refine the meanings that they encode based on the context around, all done in parallel For example, the numbers encoding the word bank might be changed based on the context surrounding it to somehow encode the more specific notion of a river bank.Transformers typically also include a second type of operation known as a feed forwardward neural network, and this gives the model extra capacity to store more patterns about language learned during training.All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage.Yeah.At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training to produce a prediction of the next word again, the model's prediction looks like a probability for every possible next word.ğŸ¼Yeah.Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does What you can see is that when you use large language model predictions to autocomplete a prompt. The words that it generates are uncannily fluent, fascinating and.Even useful.ğŸ¼If you're a new viewer and you're curious about more details on how transformers and attention work Boy do I have some material for you One option is to jump into a series I made about deep learning where we visualize and motivate the details of attention and all the other steps in a transformer but also on my second channel I just posted a talk that I gave a couple months ago about this topic for the company T and G in Munich Sometimes I actually prefer the content that I make as a casual talk rather than a produced video, but I leave it up to you which one of,Fel like the better follow on.</code>
</pre> </details>


åŸæ–‡æœ¬è¾ƒé•¿ï¼Œå‘å³æ»‘åŠ¨å¯æŸ¥çœ‹æ›´å¤š

ä½¿ç”¨DeepSeekæ•´ç†åï¼Œè¾“å‡ºå¦‚ä¸‹ï¼ˆçº¯æ–‡æœ¬è¾“å‡ºï¼Œå¯åˆ‡æ¢Markdownæ ¼å¼è¾“å‡ºï¼‰ï¼š

```markdown
è®²åº§å†…å®¹æ‘˜è¦ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†
1. æ ¸å¿ƒä¸»é¢˜
æœ¬æ¬¡è®²åº§çš„æ ¸å¿ƒä¸»é¢˜æ˜¯**å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰**çš„å·¥ä½œåŸç†ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¦‚ä½•é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ¥ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚è®²åº§è¯¦ç»†ä»‹ç»äº†æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€ç»“æ„è®¾è®¡ä»¥åŠå¦‚ä½•é€šè¿‡å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºæ¥å®ç°è¿™äº›åŠŸèƒ½ã€‚

2. ä¸»è¦çŸ¥è¯†ç‚¹ä¸ç»“æ„
2.1 è¯­è¨€æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½
é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯é¢„æµ‹ç»™å®šæ–‡æœ¬çš„ä¸‹ä¸€ä¸ªè¯ã€‚å®ƒä»¬é€šè¿‡ä¸ºæ‰€æœ‰å¯èƒ½çš„è¯åˆ†é…æ¦‚ç‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè€Œä¸æ˜¯ç¡®å®šæ€§åœ°é€‰æ‹©ä¸€ä¸ªè¯ã€‚
ç”Ÿæˆè‡ªç„¶è¯­è¨€ï¼šé€šè¿‡åå¤é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆè¿è´¯çš„å¯¹è¯æˆ–æ–‡æœ¬ã€‚ä¸ºäº†ä½¿è¾“å‡ºæ›´è‡ªç„¶ï¼Œæ¨¡å‹æœ‰æ—¶ä¼šéšæœºé€‰æ‹©æ¦‚ç‡è¾ƒä½çš„è¯ã€‚
2.2 æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹
é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰ï¼š
æ¨¡å‹é€šè¿‡å¤„ç†å¤§é‡æ–‡æœ¬ï¼ˆé€šå¸¸æ¥è‡ªäº’è”ç½‘ï¼‰æ¥å­¦ä¹ é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚
ä¾‹å¦‚ï¼Œè®­ç»ƒGPT-3æ‰€ç”¨çš„æ–‡æœ¬é‡ï¼Œå¦‚æœä¸€ä¸ªäºº24/7ä¸é—´æ–­é˜…è¯»ï¼Œéœ€è¦è¶…è¿‡2,600å¹´æ‰èƒ½å®Œæˆã€‚
è®­ç»ƒè¿‡ç¨‹æ¶‰åŠè°ƒæ•´æ¨¡å‹çš„å‚æ•°ï¼ˆparameters/weightsï¼‰ï¼Œè¿™äº›å‚æ•°å†³å®šäº†æ¨¡å‹çš„è¡Œä¸ºã€‚
é€šè¿‡**åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰**ç®—æ³•ï¼Œæ¨¡å‹ä¸æ–­ä¼˜åŒ–å‚æ•°ï¼Œä½¿å…¶æ›´å¯èƒ½é¢„æµ‹æ­£ç¡®çš„è¯ã€‚
å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆReinforcement Learning with Human Feedback, RLHFï¼‰ï¼š
ä¸ºäº†ä½¿æ¨¡å‹æ›´é€‚åˆä½œä¸ºAIåŠ©æ‰‹ï¼Œæ¨¡å‹ä¼šç»è¿‡é¢å¤–çš„è®­ç»ƒé˜¶æ®µã€‚äººç±»å·¥ä½œè€…æ ‡è®°ä¸å‡†ç¡®æˆ–ä¸åˆé€‚çš„é¢„æµ‹ï¼Œå¹¶ä¿®æ­£æ¨¡å‹ï¼Œä½¿å…¶æ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚ã€‚
2.3 æ¨¡å‹çš„è§„æ¨¡ä¸è®¡ç®—éœ€æ±‚
å‚æ•°æ•°é‡ï¼šå¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸æœ‰æ•°ç™¾äº¿ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°é€šè¿‡è®­ç»ƒä¸æ–­è°ƒæ•´ã€‚
è®¡ç®—è§„æ¨¡ï¼šè®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚ä¾‹å¦‚ï¼Œå®Œæˆæœ€å¤§æ¨¡å‹çš„è®­ç»ƒæ“ä½œï¼Œå³ä½¿æ¯ç§’è¿›è¡Œ10äº¿æ¬¡è¿ç®—ï¼Œä¹Ÿéœ€è¦è¶…è¿‡1äº¿å¹´ã€‚
ç¡¬ä»¶æ”¯æŒï¼šè®­ç»ƒè¿‡ç¨‹ä¾èµ–äºGPUï¼ˆå›¾å½¢å¤„ç†å•å…ƒï¼‰ï¼Œè¿™äº›èŠ¯ç‰‡èƒ½å¤Ÿå¹¶è¡Œå¤„ç†å¤§é‡è¿ç®—ã€‚
2.4 Transformeræ¶æ„
å¹¶è¡Œå¤„ç†ï¼šä¸æ—©æœŸè¯­è¨€æ¨¡å‹é€è¯å¤„ç†ä¸åŒï¼ŒTransformeræ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†æ•´ä¸ªæ–‡æœ¬ã€‚
è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰ï¼šæ¯ä¸ªè¯è¢«ç¼–ç ä¸ºä¸€ä¸ªé•¿åˆ—è¡¨çš„æ•°å­—ï¼Œè¿™äº›æ•°å­—é€šè¿‡è®­ç»ƒä¸æ–­è°ƒæ•´ï¼Œä»¥æ•æ‰è¯çš„è¯­ä¹‰ã€‚
æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ï¼š
Transformerçš„æ ¸å¿ƒæ“ä½œæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå…è®¸æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´è¯çš„ç¼–ç ã€‚
ä¾‹å¦‚ï¼Œè¯â€œbankâ€çš„ç¼–ç ä¼šæ ¹æ®ä¸Šä¸‹æ–‡ï¼ˆå¦‚â€œriver bankâ€æˆ–â€œbank accountâ€ï¼‰è¿›è¡Œè°ƒæ•´ã€‚
å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼‰ï¼šTransformerè¿˜åŒ…æ‹¬å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå­˜å‚¨æ›´å¤šè¯­è¨€æ¨¡å¼ã€‚
2.5 æ¨¡å‹çš„é¢„æµ‹ä¸è¾“å‡º
æœ€ç»ˆé¢„æµ‹ï¼šåœ¨Transformerçš„æœ€åä¸€æ­¥ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥æ–‡æœ¬å’Œè®­ç»ƒä¸­å­¦åˆ°çš„çŸ¥è¯†ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚
æ¶Œç°è¡Œä¸ºï¼ˆEmergent Behaviorï¼‰ï¼šæ¨¡å‹çš„å…·ä½“è¡Œä¸ºæ˜¯ç”±æ•°ç™¾äº¿ä¸ªå‚æ•°åœ¨è®­ç»ƒä¸­è°ƒæ•´çš„ç»“æœï¼Œè¿™ä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å®Œå…¨è§£é‡Šæ¨¡å‹çš„é¢„æµ‹é€»è¾‘ã€‚

3. é‡è¦ç»†èŠ‚
è®­ç»ƒæ•°æ®é‡ï¼šGPT-3çš„è®­ç»ƒæ•°æ®é‡ç›¸å½“äºä¸€ä¸ªäººé˜…è¯»2,600å¹´çš„æ–‡æœ¬ã€‚
è®¡ç®—æ—¶é—´ï¼šè®­ç»ƒæœ€å¤§æ¨¡å‹æ‰€éœ€çš„è®¡ç®—æ—¶é—´è¶…è¿‡1äº¿å¹´ï¼ˆå‡è®¾æ¯ç§’10äº¿æ¬¡è¿ç®—ï¼‰ã€‚
æ³¨æ„åŠ›æœºåˆ¶çš„ä½œç”¨ï¼šæ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´è¯çš„è¯­ä¹‰ç¼–ç ã€‚
å¼ºåŒ–å­¦ä¹ çš„ä½œç”¨ï¼šé€šè¿‡äººç±»åé¦ˆï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„å“åº”ã€‚

4. æ‹“å±•ä¸è¡¥å……
Transformerçš„èµ·æºï¼šTransformeræ¶æ„ç”±Googleçš„ç ”ç©¶å›¢é˜Ÿäº2017å¹´æå‡ºï¼Œå½»åº•æ”¹å˜äº†è¯­è¨€æ¨¡å‹çš„å¤„ç†æ–¹å¼ã€‚
æ³¨æ„åŠ›æœºåˆ¶çš„å¯è§†åŒ–ï¼šè®²åº§æåˆ°å¯ä»¥é€šè¿‡æ·±åº¦å­¦ä¹ ç³»åˆ—è§†é¢‘è¿›ä¸€æ­¥äº†è§£æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†ã€‚

5. æ€»ç»“
æœ¬æ¬¡è®²åº§æ·±å…¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œä»é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„åŸºæœ¬åŠŸèƒ½åˆ°å¤æ‚çš„è®­ç»ƒè¿‡ç¨‹å’ŒTransformeræ¶æ„çš„è®¾è®¡ã€‚é€šè¿‡å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæµç•…ä¸”è‡ªç„¶çš„è¯­è¨€æ–‡æœ¬ï¼Œä½†å…¶å…·ä½“è¡Œä¸ºä»ç„¶æ˜¯ä¸€ä¸ªå¤æ‚çš„æ¶Œç°ç°è±¡ï¼Œéš¾ä»¥å®Œå…¨è§£é‡Šã€‚
```

### å¿«é€Ÿå¼€å§‹

#### å®¢æˆ·ç«¯

cloneæœ¬é¡¹ç›®ï¼Œå®‰è£…ä¾èµ–ï¼Œè¿è¡Œå³å¯

```bash
git clone https://github.com/FrankLightcone/404NotSound.git
cd 404NotSound
pip install -r requirements.txt
python app.py
```
#### æœåŠ¡å™¨ç«¯
éœ€è¦å®‰è£…SenseVoiceç›¸å…³ä¾èµ–ï¼Œå‚è€ƒ[SenseVoice](https://github.com/FunAudioLLM/SenseVoice)çš„å®‰è£…æ­¥éª¤

è¿è¡ŒSenseVoiceæœåŠ¡ç«¯ï¼š
```bash
python api_key_server.py
```
æ ¹æ®å…·ä½“æƒ…å†µåœ¨å®¢æˆ·ç«¯ä¸­å¡«å†™SenseVoiceçš„APIåœ°å€ï¼Œé¦–æ¬¡è¿è¡Œä¼šè¾“å‡ºç®¡ç†å‘˜API Keyï¼ŒæŒæœ‰æ­¤Key å¯ä»¥é€šè¿‡å¦‚ä¸‹å‘½ä»¤åˆ›å»ºæ–°çš„API Keyï¼š
```bash
curl -k -X POST "https://<Your Server IP Address>:14612/admin/create_key" \
     -H "X-API-Key: <Your Server Admin API Key>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

å…³äºDeepSeekç›¸å…³é—®é¢˜ï¼Œå‚è€ƒ[DeepSeek API](https://api-docs.deepseek.com/)æ–‡æ¡£


### å¼€å‘è®¡åˆ’

- [x] å®ŒæˆåŸºæœ¬çš„GUIç•Œé¢
- [ ] æ”¯æŒè§†é¢‘è¾“å…¥
- [ ] æ”¯æŒåŒæ—¶è¾“å…¥è¯¾ç¨‹PPT
- [ ] æ”¯æŒä½¿ç”¨å…¶ä»–LLMæ¨¡å‹
- [ ] æ”¯æŒå¤šè½®å¯¹è¯